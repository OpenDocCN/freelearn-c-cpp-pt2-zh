["```cpp\nstd::mutex m;\nsize_t count;// Guarded by m\n\u2026 on the threads \u2026 \n{\n\u00a0\u00a0std::lock_guard l(m);\n\u00a0\u00a0++count;\n}\n```", "```cpp\nstd::atomic<size_t> count;\n\u2026 on the threads \u2026 \n++count;\n```", "```cpp\nstd::atomic<size_t> count;\n\u2026 on the threads \u2026 \ncount.fetch_add(1, std::memory_order_relaxed);\n```", "```cpp\nstd::atomic<size_t> count;\n\u2026 on the threads \u2026 \nsize_t c = count.load(std::memory_order_relaxed);\nwhile (!count.compare_exchange_strong(c, c + 1,\n\u00a0\u00a0\u00a0\u00a0\u00a0std::memory_order_relaxed, std::memory_order_relaxed)) {}\n```", "```cpp\nstd::mutex m;\nsize_t count = 0; \nvoid BM_lock(benchmark::State& state) {\n\u00a0\u00a0if (state.thread_index == 0) count = 0;\n\u00a0\u00a0for (auto _ : state) {\n\u00a0\u00a0\u00a0\u00a0std::lock_guard l(m);\n\u00a0\u00a0\u00a0\u00a0++count;\n\u00a0\u00a0}\u00a0\u00a0\u00a0\n}\nBENCHMARK(BM_lock)->Threads(2)->UseRealTime();\n```", "```cpp\nclass Spinlock {\n\u00a0\u00a0public:\n\u00a0\u00a0void lock() {\n\u00a0\u00a0\u00a0\u00a0while (flag_.exchange(1, std::memory_order_acquire)) {}\n\u00a0\u00a0}\n\u00a0\u00a0void unlock() { flag_.store(0, std::memory_order_release); }\n\u00a0\u00a0private:\n\u00a0\u00a0std::atomic<unsigned int> flag_;\n};\n```", "```cpp\nclass Spinlock {\n\u00a0\u00a0void lock() {\n\u00a0\u00a0\u00a0\u00a0while (flag_.load(std::memory_order_relaxed) ||\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0flag_.exchange(1, std::memory_order_acquire)) {}\n\u00a0\u00a0}\n}\n```", "```cpp\nclass Spinlock {\n\u00a0\u00a0void lock() {\n\u00a0\u00a0\u00a0\u00a0for (int i=0; flag_.load(std::memory_order_relaxed) ||\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0flag_.exchange(1, std::memory_order_acquire); ++i) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if (i == 8) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0lock_sleep();\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0i = 0;\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0}\n\u00a0\u00a0void lock_sleep() {\n\u00a0\u00a0\u00a0\u00a0static const timespec ns = { 0, 1 }; // 1 nanosecond\n\u00a0\u00a0\u00a0\u00a0nanosleep(&ns, NULL);\n\u00a0\u00a0}\n}\n```", "```cpp\ntemplate <typename T>\nclass PtrSpinlock {\n\u00a0\u00a0public:\n\u00a0\u00a0explicit PtrSpinlock(T* p) : p_(p) {}\n\u00a0\u00a0T* lock() {\n\u00a0\u00a0\u00a0\u00a0while (!(saved_p_ = \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0p_.exchange(nullptr, std::memory_order_acquire))) {}\n\u00a0\u00a0}\n\u00a0\u00a0void unlock() { \n\u00a0\u00a0\u00a0\u00a0p_.store(saved_p_, std::memory_order_release); \n\u00a0\u00a0}\n\u00a0\u00a0private:\n\u00a0\u00a0std::atomic<T*> p_;\n\u00a0\u00a0T* saved_p_ = nullptr;\n};\n```", "```cpp\nclass AtomicIndex {\n\u00a0\u00a0std::atomic<unsigned long> c_;\n\u00a0\u00a0public:\n\u00a0\u00a0unsigned long incr() noexcept {\n\u00a0\u00a0\u00a0\u00a0return 1 + c_.fetch_add(1, std::memory_order_release); \n\u00a0\u00a0}\n\u00a0\u00a0unsigned long get() const noexcept { \n\u00a0\u00a0\u00a0\u00a0return c_.load(std::memory_order_acquire);\n\u00a0\u00a0}\n};\n```", "```cpp\nclass AtomicCount {\n\u00a0\u00a0std::atomic<unsigned long> c_;\n\u00a0\u00a0public:\n\u00a0\u00a0unsigned long incr() noexcept {\n\u00a0\u00a0\u00a0\u00a0return 1 + c_.fetch_add(1, std::memory_order_relaxed); \n\u00a0\u00a0}\n\u00a0\u00a0unsigned long get() const noexcept { \n\u00a0\u00a0\u00a0\u00a0return c_.load(std::memory_order_relaxed);\n\u00a0\u00a0}\n};\n```", "```cpp\ntemplate <typename T>\nclass ts_unique_ptr {\n\u00a0\u00a0public:\n\u00a0\u00a0ts_unique_ptr() = default;\n\u00a0\u00a0explicit ts_unique_ptr(T* p) : p_(p) {}\n\u00a0\u00a0ts_unique_ptr(const ts_unique_ptr&) = delete;\n\u00a0\u00a0ts_unique_ptr& operator=(const ts_unique_ptr&) = delete;\n\u00a0\u00a0~ts_unique_ptr() {\n\u00a0\u00a0\u00a0\u00a0delete p_.load(std::memory_order_relaxed);\n\u00a0\u00a0}\n\u00a0\u00a0void publish(T* p) noexcept {\n\u00a0\u00a0\u00a0\u00a0p_.store(p, std::memory_order_release);\n\u00a0\u00a0}\n\u00a0\u00a0const T* get() const noexcept {\n\u00a0\u00a0\u00a0\u00a0return p_.load(std::memory_order_acquire);\n\u00a0\u00a0}\n\u00a0\u00a0const T& operator*() const noexcept { return *this->get(); }\n\u00a0\u00a0ts_unique_ptr& operator=(T* p) noexcept {\n\u00a0\u00a0\u00a0\u00a0this->publish(p); return *this;\n\u00a0\u00a0}\n\u00a0\u00a0private:\n\u00a0\u00a0std::atomic<T*> p_ { nullptr };\n};\n```", "```cpp\nstruct A { \u2026 arbitrary object for testing \u2026 };\nts_unique_ptr<A> p(new A(\u2026));\nvoid BM_ptr_deref(benchmark::State& state) {\n\u00a0\u00a0A x;\n\u00a0\u00a0for (auto _ : state) {\n\u00a0\u00a0\u00a0\u00a0benchmark::DoNotOptimize(x = *p);\n\u00a0\u00a0}\n\u00a0\u00a0state.SetItemsProcessed(state.iterations());\n}\nBENCHMARK(BM_ptr_deref)->Threads(1)->UseRealTime();\n\u2026 repeat for desired number of threads \u2026 \nBENCHMARK_MAIN();\n```", "```cpp\nstd::shared_ptr<T> p_;\nT* data = new T;\n\u2026 finish initializing the data \u2026\nstd::atomic_store_explicit(\n\u00a0\u00a0\u00a0\u00a0&p_, std::shared_ptr<T>(data), std::memory_order_release);\n```", "```cpp\nstd::shared_ptr<T> p_;\nconst T* data = std::atomic_load_explicit(\n\u00a0\u00a0\u00a0\u00a0&p_, std::memory_order_acquire).get();\n```", "```cpp\ntemplate <typename T> struct Wrapper {\n\u00a0\u00a0T object;\n\u00a0\u00a0Wrapper(\u2026 arguments \u2026) : object(\u2026) {}\n\u00a0\u00a0~Wrapper() = default;\n\u00a0\u00a0Wrapper (const Wrapper&) = delete;\n\u00a0\u00a0Wrapper& operator=(const Wrapper&) = delete;\n\u00a0\u00a0std::atomic<size_t> ref_cnt_ = 0;\n\u00a0\u00a0void AddRef() {\n\u00a0\u00a0\u00a0\u00a0ref_cnt_.fetch_add(1, std::memory_order_acq_rel);\n\u00a0\u00a0}\n\u00a0\u00a0bool DelRef() { return\n\u00a0\u00a0\u00a0\u00a0ref_cnt_.fetch_sub(1, std::memory_order_acq_rel) == 1;\n\u00a0\u00a0}\n};\n```"]