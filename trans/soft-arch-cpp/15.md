Cloud-Native Design

顾名思义，云原生设计描述了应用程序的架构，首先是为了在云中运行而构建的。它不是由单一的技术或语言定义的，而是利用了现代云平台提供的一切。

这可能意味着在必要时使用**平台即服务** (**PaaS**) 的组合，多云部署，边缘计算，**功能即服务** (**FaaS**) 、静态文件托管、微服务和托管服务。它超越了传统操作系统的界限。云原生开发人员不是针对 POSIX API 和类 UNIX 操作系统，而是使用 boto3、Pulumi 或 Kubernetes 等库和框架构建更高级别的概念。

本章将介绍以下主题:

*   理解云-原生
*   使用 Kubernetes 协调云原生工作负载
*   使用服务网格连接服务
*   分布式系统中的可观测性
*   去 GitOps

在本章结束时，您将对如何在应用程序中使用软件体系结构的现代趋势有很好的了解。

# 技术要求

本章中的一些示例需要 Kubernetes 1.18。

本章中出现的代码已放在 GitHub 上，https://github.com/PacktPublishing/ 软件架构与 Cpp/tree/master/Chapter15。

# 理解云-原生

尽管可以将现有应用程序迁移到云中运行，但这种迁移不会使应用程序成为本地云应用程序。它将在云中运行，但架构选择仍将基于内部部署模型。

简而言之，云原生应用程序本质上是分布式的，松散耦合的，并且具有可伸缩性。它们与任何特定的物理基础架构无关，甚至不需要开发人员考虑特定的基础架构。此类应用程序通常以 web 为中心。

在本章中，我们将介绍一些云原生构建块的示例，并描述一些云原生模式。

## 云原生计算基础

云原生设计的一个支持者是云原生计算基础 (**CNCF**)，它承载着 Kubernetes 项目。CNCF 拥有各种技术，因此可以更轻松地构建独立于云供应商的云原生应用程序。此类技术的示例包括:

*   **Fluentd**，统一的测井层
*   **Jaeger**，用于分布式跟踪
*   **普罗米修斯**，用于监控
*   **CoreDNS**，用于服务发现

云原生应用程序通常使用应用程序容器构建，通常在 Kubernetes 平台之上运行。但是，这不是必需的，完全可以在 Kubernetes 和容器之外使用许多 CNCF 框架。

## 云作为操作系统

云原生设计的主要特点是将各种云资源视为应用程序的构建块。单独的**虚拟机** (**VMs**) 很少在云原生设计中使用。您可以直接定位云 API (例如，使用 FaaS) 或某些中介解决方案 (例如 Kubernetes)，而不是定位在某些实例上运行的给定操作系统。从这个意义上讲，云成为您的操作系统，因为 POSIX API 不再限制您。

随着容器改变了构建和分发软件的方法，现在可以使自己摆脱对底层硬件基础结构的思考。您的软件不是隔离地工作，因此仍然需要连接不同的服务，监视它们，控制它们的生命周期，存储数据或传递秘密。这是 Kubernetes 提供的东西，也是它变得如此流行的原因之一。

正如你可能想象的那样，云原生应用程序是网络和移动优先的。桌面应用程序也可以从拥有一些云原生组件中受益，但这是一个不太常见的用例。

在云原生应用程序中仍然可以使用硬件和其他低级访问。如果您的工作负载需要使用 GPU，这不应阻止您使用云原生。此外，如果您希望访问其他地方不可用的自定义硬件，则可以在本地构建云原生应用程序。该术语不仅限于公共云，而是考虑不同资源的方式。

### 负载平衡和服务发现

负载平衡是分布式应用程序的重要组成部分。它不仅将传入的请求分散到服务集群中，这对于扩展至关重要，而且还可以帮助提高应用程序的响应能力和可用性。智能负载平衡器可以收集度量标准，以对传入流量中的模式做出反应，监视其群集中服务器的状态，并将请求转发到负载较少且响应速度更快的节点，从而避免当前不健康的节点。

负载平衡带来更多的吞吐量和更少的停机时间。通过将请求转发到许多服务器，消除了单点故障，尤其是在使用多个负载平衡器的情况下，例如在主动-被动方案中。

负载平衡器可以在您的架构中的任何地方使用: 您可以平衡来自 web 的请求，web 服务器对其他服务的请求，对缓存或数据库服务器的请求以及其他任何适合您的要求。

There are a few things to remember when introducing load balancing. One of them is session persistence—make sure all requests from the same customer go to the same server, so the carefully chosen pink stilettos won't disappear from their basket in your e-commerce site. Sessions can get tricky with load balancing: take extra care to not mix sessions, so customers won't suddenly start being logged into each other's profiles – countless companies stumbled upon this error before, especially when adding caching into the mix. It's a great idea to combine the two; just make sure it is done the right way.

#### 反向代理

即使您只想部署服务器的一个实例，也可以在其前面添加另一个服务而不是负载均衡器-反向代理。虽然代理通常代表发送某些请求的客户端起作用，但反向代理代表处理这些请求的服务器起作用，因此得名。

你问，为什么要用它？这样的代理有几个原因和用途:

*   **安全性**: 您的服务器地址现在是隐藏的，服务器可以通过代理的 DDoS 防护能力进行防护。
*   **灵活性和可伸缩性**: 您可以根据需要随时以任何方式修改隐藏在代理后面的基础架构。
*   **缓存**: 如果你已经知道服务器会给出什么答案，为什么还要麻烦服务器呢？
*   **压缩**: 压缩数据会减少所需的带宽，这对于连接性较差的移动用户可能特别有用。它还可以降低您的网络成本 (但可能会降低您的计算能力)。
*   **SSL 终止**: 通过承担网络流量加密和解密的负担来降低后端服务器的负载。

反向代理的一个例子是**NGINX**。它还提供负载平衡功能，A/B 测试等。它的其他功能之一是服务发现。让我们看看它有什么帮助。

#### 服务发现

顾名思义，**服务发现** (**SD**) 允许自动检测计算机网络中特定服务的实例。调用方必须仅指向服务注册中心，而不是对应该托管服务的域名或 IP 进行硬编码。使用这种方法，您的体系结构变得更加灵活，因为现在您使用的所有服务都可以轻松找到。如果您设计基于微服务的体系结构，那么引入 SD 确实会走很长一段路。

有几种方法可以实现 SD。在客户端发现中，调用方直接联系 SD 实例。每个服务实例都有一个注册表客户端，用于注册和注销实例，处理心跳等。尽管非常简单，但在这种方法中，每个客户端都必须实现服务发现逻辑。Netflix Eureka 是这种方法中常用的服务注册中心的一个例子。

一种替代方法是使用服务器端发现。在这里，服务注册表以及每个服务实例中的注册表客户端也存在。但是，呼叫者不会直接与之联系。相反，它们连接到负载平衡器，例如 AWS Elastic load balancer，后者在将客户端调用分派到特定实例之前，调用服务注册表或使用其内置的服务注册表。除了 AWS ELB 之外，NGINX 和 Consul 还可以用于提供服务器端 SD 功能。

我们现在知道如何有效地找到和使用我们的服务，所以让我们学习如何最好地部署它们。

# 使用 Kubernetes 协调云原生工作负载

Kubernetes 是一个可扩展的开源平台，用于自动化和管理容器应用程序。有时被称为 k8s，因为它以 “k” 开头，以 “s” 结尾，中间有八个字母。

它的设计基于 Borg，一个 Google 内部使用的系统。Kubernetes 中存在的一些功能如下:

*   应用程序的自动缩放
*   可配置网络
*   批处理作业执行
*   统一升级应用
*   在其之上运行高可用应用程序的能力
*   声明式配置

在您的组织中运行 Kubernetes 有不同的方法。选择一个而不是另一个需要你分析与之相关的额外成本和收益。

## Kubernetes 结构

虽然可以在一台机器上运行 Kubernetes (例如，使用 minikube、k3s 或 k3d)，但不建议在生产中这样做。单机集群的功能有限，没有故障转移机制。Kubernetes 集群的典型大小是六台或以上。然后，其中三台机器形成控制平面。另外三个是 worker 节点。

三台机器的最低要求来自这样一个事实，即这是提供高可用性的最低数量。可以将控制平面节点也用作工作节点，尽管不鼓励这样做。

### 控制平面

在 Kubernetes 中，您很少与单个工作节点进行交互。相反，所有的 API 请求都转到控制平面。然后，控制平面根据请求决定要采取的行动，然后与工作节点通信。

与控制平面的交互可以采取几种形式:

*   使用 kubectl CLI
*   使用 web 仪表板
*   从 kubectl 以外的应用程序内部使用 Kubernetes API

控制平面节点通常运行 API 服务器，调度程序，配置存储 (etcd) 以及可能的一些其他进程来处理特定需求。例如，部署在公共云 (例如 Google cloud Platform) 中的 Kubernetes 集群具有运行在控制平面节点上的云控制器。云控制器与云提供商的 API 交互，以替换发生故障的计算机、配置负载平衡器或分配外部 ip 地址。

### 工作节点

构成控制平面和工作池的节点是工作负载将在其上运行的实际计算机。它们可能是您在本地托管的物理服务器、私人托管的 vm 或来自云提供商的 vm。

群集中的每个节点至少运行以下三个程序:

*   允许机器处理应用程序容器的容器运行时 (例如，Docker 引擎或 cri-o)
*   kubelet，负责接收来自控制平面的请求，并根据这些请求管理各个容器
*   kube-proxy，负责节点级别的网络和负载平衡

## 部署 Kubernetes 的可能方法

正如您从阅读上一节中可能意识到的那样，部署 Kubernetes 有不同的可能方法。

其中之一是将其部署到本地托管的裸机服务器。好处之一是，对于大规模应用程序而言，这可能比云提供商提供的价格便宜。这种方法有一个主要缺点-您将需要操作员在必要时提供额外的节点。

为了缓解此问题，您可以在裸机服务器上运行虚拟化设备。这使得可以使用 Kubernetes 内置的云控制器自动提供必要的资源。您仍然可以控制成本，但是手工工作较少。虚拟化增加了一些开销，但在大多数情况下，这应该是一个公平的权衡。

如果您对自己托管服务器不感兴趣，则可以部署 Kubernetes 以在云提供商的 vm 上运行。通过选择此路线，您可以使用一些现有模板进行最佳设置。有 Terraform 和 Ansible 模块可用于在流行的云平台上构建集群。

最后，还有主要云玩家提供的托管服务。您只需支付其中一些工作节点的费用，而控制平面是免费的。

在公共云中运行时，为什么要选择自托管的 Kubernetes 而不是托管服务？原因之一可能是您需要的 Kubernetes 的特定版本。云提供商在引入更新时通常会有点慢。

## 理解 Kubernetes 的概念

Kubernetes 引入了一些听起来可能不熟悉或令人困惑的概念，如果您是第一次听到它们。当您了解它们的目的时，应该更容易掌握使 Kubernetes 与众不同的原因。以下是一些最常见的 Kubernetes 对象:

*   *容器*，具体来说就是应用容器，是一种分发和运行单个应用的方法。它包含在任何地方运行未修改的应用程序所需的代码和配置。
*   *Pod*是基本的 Kubernetes 构建块。它是原子的，由一个或多个容器组成。pod 内部的所有容器共享相同的网络接口，卷 (例如持久存储或秘密) 和资源 (CPU 和内存)。
*   *部署*是描述工作负载及其生命周期特征的更高级别的对象。它通常管理一组 pod 副本，允许滚动升级，并在出现故障时管理回滚。这就是使扩展和管理 Kubernetes 应用程序生命周期变得容易的原因。
*   *DaemonSet*是一个类似于部署的控制器，它管理 pod 的分布位置。虽然部署涉及保持给定数量的副本，但守护进程集将 pod 分散到所有工作节点上。主要用例是在每个节点上运行系统级服务，例如监视或日志记录代理。
*   *作业*是为一次性任务而设计的。当部署中的容器终止时，部署中的 pod 会自动重新启动。它们适用于在网络端口上侦听请求的所有始终运行的服务。但是，部署不适合批处理作业，例如仅在需要时才运行缩略图生成。作业创建一个或多个 pod 并观察它们，直到它们完成给定的任务。当特定数量的成功 pod 终止时，作业被视为完成。
*   *CronJobs*，顾名思义，是集群内周期性运行的作业。
*   *服务*表示集群内执行的特定功能。它们有一个与它们相关联的网络端点 (通常是负载平衡的)。服务可以由一个或多个 pod 执行。服务的生命周期与许多 pod 的生命周期无关。由于 pod 是短暂的，因此它们可能随时被创建和销毁。服务抽象了各个 pod，以实现高可用性。为了便于使用，服务有自己的 ip 地址和 DNS 名称。

### 声明性方法

我们已经在[第 9 章](https://cdp.packtpub.com/hands_on_software_architecture_with_c__/wp-admin/post.php?post=33&action=edit)，*连续集成/连续部署*[中介绍了声明性方法和命令性方法之间的区别。](https://cdp.packtpub.com/hands_on_software_architecture_with_c__/wp-admin/post.php?post=33&action=edit)Kubernetes 采用声明式方法。您无需提供有关需要采取的步骤的说明，而是提供描述群集所需状态的资源。由控制平面来分配内部资源，以便它们满足您的需求。

可以直接使用命令行添加资源。这可以很快进行测试，但是您希望对大多数时间创建的资源进行跟踪。因此，大多数人使用清单文件，这些文件提供所需资源的编码描述。清单通常是 YAML 文件，但也可以使用 JSON。

下面是一个带有单个 Pod 的 YAML 清单示例:

```cpp
apiVersion: v1
kind: Pod
metadata:
  name: simple-server
  labels:
    app: dominican-front
spec:
  containers:
    - name: webserver
      image: nginx
      ports:
        - name: http
          containerPort: 80
          protocol: TCP
```

第一行是强制性的，它告诉清单中将使用哪个 API 版本。有些资源仅在扩展中可用，因此这是解析器有关如何行为的信息。

第二行描述了我们正在创建什么资源。接下来，还有元数据和资源的规范。

在元数据中必须使用名称，因为这是区分一种资源与另一种资源的方法。如果我们想创建另一个具有相同名称的 pod，则会出现错误，指出该资源已经存在。标签是可选的，在编写选择器时很有用。例如，如果我们想创建一个允许连接到 pod 的服务，我们将使用一个选择器匹配标签应用程序，其值等于`dominican-front`。

规范也是强制性的部分，因为它描述了资源的实际内容。在我们的示例中，我们列出了 pod 内部运行的所有容器。准确地说，一个名为`webserver`的容器使用 Docker Hub 中的图像`nginx`。由于我们希望从外部连接到 Nginx web 服务器，因此我们还公开了服务器正在侦听的容器端口`80`。端口描述中的名称是可选的。

## Kubernetes 网络

Kubernetes 允许可插拔网络体系结构。存在几个驱动程序，可以根据需求使用。无论你选择哪个驱动程序，有些概念都是通用的。以下是典型的联网场景。

### 集装箱到集装箱通信

单个 pod 可以容纳几个不同的容器。由于网络接口与 pod 绑定，而不是与容器绑定，因此每个容器都在同一网络命名空间中运行。这意味着各种容器可以使用本地主机网络相互寻址。

### Pod 到 pod 通信

每个 pod 都有一个内部集群-分配的本地 ip 地址。一旦 pod 被删除，该地址就不会持续存在。当一个 pod 知道另一个 pod 共享相同的扁平网络时，它可以连接到另一个 pod 的暴露端口。关于这种通信模型，您可以将 pod 视为托管容器的 vm。这很少使用，因为首选方法是 pod 到服务通信。

### Pod 到服务通信

Pod 到服务通信是集群内最流行的通信用例。每个服务都有一个单独的 ip 地址和分配给它的 DNS 名称。当 pod 连接到服务时，该连接将被代理到服务选择的组中的一个 pod。代理是前面描述的 kube-proxy 工具的一项任务。

### 外部到内部沟通

外部流量通常通过负载平衡器进入集群。这些要么与特定服务或入口控制器绑定，要么由其处理。当外部暴露的服务处理流量时，它的行为类似于 pod 到服务的通信。使用 ingress 控制器，您可以使用其他功能来实现路由，可观察性或高级负载平衡。

## 什么时候使用 Kubernetes 是个好主意？

在组织中引入 Kubernetes 需要一些投资。Kubernetes 提供了许多好处，例如可自动缩放，自动化或部署场景。然而，这些好处可能无法证明必要的投资是合理的。

这项投资涉及几个领域:

*   **基础设施成本**: 与运行控制平面和 worker 节点相关的成本可能相对较高。此外，如果您想使用各种 Kubernetes 扩展，例如 GitOps 或 service mesh (稍后描述)，则成本可能会增加。它们还需要额外的资源来运行，并在应用程序的常规服务之上提供更多的开销。除了节点本身，您还应该考虑其他成本。一些 Kubernetes 功能在部署到受支持的云提供商时效果最好。这意味着，为了从这些功能中受益，您必须走以下路线之一:

a.将您的工作负载移动到特别支持的云。

b.为您选择的云提供商实现您自己的驱动程序。

c.将您的内部部署基础架构迁移到支持虚拟化 API 的环境，如 VMware vSphere 或 OpenStack。

*   **运营成本**: Kubernetes 集群和关联服务需要维护。即使您获得的应用程序维护较少，但保持集群运行的成本也会略微抵消这一好处。
*   **教育成本**: 你的整个产品团队都要学习新的概念。即使你有一个专门的平台团队，为开发人员提供易于使用的工具，开发人员仍然需要对他们所做的工作如何影响整个系统以及他们应该使用哪种 API 有一个基本的了解。

在决定引入 Kubernetes 之前，请首先考虑您是否能够负担得起所需的初始投资。

# 分布式系统中的可观测性

云原生架构等分布式系统带来了一些独特的挑战。在任何给定时间工作的不同服务的数量之多，使得调查组件的性能非常不便。

在单片系统中，日志记录和性能监控通常就足够了。对于分布式系统，即使日志记录也需要设计选择。不同的组件产生不同的日志格式。那些日志必须存储在某个地方。将它们与提供它们的服务保持在一起将使在停电情况下获得大局变得很困难。此外，由于微服务可能是短暂的，因此您将希望将日志的生命周期与提供日志的服务或托管该服务的计算机的生命周期解耦。

在[C](13.html)[hapter 13](13.html)，*设计微服务*中，我们描述了统一日志记录层如何帮助管理日志。但是日志只显示系统中给定点发生了什么。要从单个事务角度查看图片，您需要采用不同的方法。

这就是追踪的地方。

## 跟踪与日志记录有何不同

追踪是一种专门的日志记录形式。它提供的信息比日志低。这可能包括所有函数调用、它们的参数、它们的大小和执行时间。它们还包含正在处理的事务的唯一 ID。这些细节使得重新组装它们成为可能，并在给定事务通过您的系统时查看其生命周期。

跟踪中存在的性能信息可帮助您发现系统中的瓶颈和次优组件。

虽然日志通常由操作员和开发人员读取，但它们往往是人类可读的。追踪没有这样的要求。要查看跟踪，您将使用专用的可视化程序。这意味着即使痕迹更详细，它们也可能比原木占用更少的空间。

下图是单个迹线的概述:

![](assets/a9bf3fd1-f598-4f0c-bf3b-247cc54a2166.png)]

Figure 15.1 – Single trace

两种服务通过网络进行通信。在*服务 A*中，我们有一个包含子跨度和单个日志的父跨度。子跨度通常对应于更深的函数调用。日志表示最小的信息。它们中的每一个都是定时的，并且可能包含额外的信息。

对*服务 B*的网络调用保留了 span 上下文。即使在另一台机器上的不同进程中执行*服务 B*，所有信息都可以在保留事务 ID 后重新组合。

我们从重新组装跟踪中获得的一条奖励信息是分布式系统中服务之间的依赖关系图。由于跟踪包含整个调用链，因此可以可视化此信息并检查意外的依赖关系。

## 选择跟踪解决方案

在实现跟踪时，有几种可能的解决方案可供选择。正如您可能想象的那样，您可以使用自托管和托管工具来检测应用程序。我们将简要描述托管的，并重点介绍自托管的。

### Jaeger 和 OpenTracing

分布式跟踪的标准之一是 Jaeger 的作者提出的 OpenTracing。Jaeger 是为云原生应用程序构建的跟踪器。它解决了监视分布式事务和传播跟踪上下文的问题。它对于以下目的很有用:

*   性能或延迟优化
*   执行根本原因分析
*   服务间依赖关系分析

OpenTracing 是一个开放标准，它提供了一个独立于所使用的跟踪器的 API。这意味着当您的应用程序使用 OpenTracing 进行检测时，您可以避免锁定到一个特定的供应商。如果在某个时候，您决定从 Jaeger 切换到 Zipkin，DataDog 或任何其他兼容的跟踪器，则不必修改整个仪器代码。

有许多与 OpenTracing 兼容的客户端库。您还可以找到许多资源，包括解释如何根据您的需求实现 API 的教程和文章。OpenTracing 正式支持以下语言:

*   走
*   JavaScript
*   Java
*   蟒蛇
*   红宝石
*   PHP
*   目标-C
*   C
*   C #

也有非官方的库可用，特定的应用程序也可以导出 OpenTracing 数据。这包括 Nginx 和 Envoy，这两个都是流行的网络代理。

Jaeger 也接受 Zipkin 格式的样本。我们将在下一节介绍 Zipkin。它的意思是，如果您 (或您的任何依赖项) 已经使用 Zipkin，则不必将仪器从一种格式重写为另一种格式。对于所有新应用程序，OpenTracing 是推荐的方法。

Jaeger 缩放得很好。如果要评估它，可以将其作为单个二进制文件或单个应用程序容器运行。您可以将 Jaeger for production 配置为使用其自己的后端或受支持的外部后端，例如 Elasticsearch，Cassandra 或 Kafka。

Jaeger 是 CNCF 毕业项目。这意味着它已经达到了与 Kubernetes，Prometheus 或 Fluentd 相似的成熟度。因此，我们希望它在其他 CNCF 应用程序中获得更多支持。

### Zipkin

积家的主要竞争对手是齐普金。这是一个较旧的项目，这也意味着它更加成熟。通常，更高级的项目也得到更好的支持，但是在这种情况下，CNCF 的认可对 Jaeger 有利。

Zipkin 使用其专有协议来处理跟踪。它具有可用的 OpenTracing 支持，但可能与本机 Jaeger 协议的成熟度和支持级别不同。正如我们前面提到的，也可以配置 Jaeger 以 Zipkin 格式收集跟踪。这意味着两者至少在某种程度上是可互换的。

该项目托管在 Apache 基金会下，但不被视为 CNCF 项目。在开发云原生应用程序时，Jaeger 是一个更好的选择。如果您正在寻找通用的跟踪解决方案，那么也值得考虑 Zipkin。

一个缺点是 Zipkin 没有受支持的 C 实现。有非官方的图书馆，但它们似乎没有得到很好的支持。使用 C OpenTracing 库是使用 C 代码的首选方法。

## 用 OpenTracing 为应用程序提供工具

本节将说明如何将带有 Jaeger 和 OpenTracing 的 instrumentation 添加到现有应用程序中。我们将使用`opentracing-cpp`和`jaeger-client-cpp`库。

首先，我们要设置跟踪器:

```cpp
#include <jaegertracing/Tracer.h>

void setUpTracer()
{
    // We want to read the sampling server configuration from the 
    // environment variables
    auto config = jaegertracing::Config;
    config.fromEnv();
    // Jaeger provides us with ConsoleLogger and NullLogger
    auto tracer = jaegertracing::Tracer::make(
        "customer", config, jaegertracing::logging::consoleLogger());
    opentracing::Tracer::InitGlobal(
        std::static_pointer_cast<opentracing::Tracer>(tracer));
}
```

配置采样服务器的两种首选方法要么像我们一样使用环境变量，要么使用 YAML 配置文件。使用环境变量时，我们将必须在运行应用程序之前对其进行设置。最重要的如下:

*   `JAEGER_AGENT_HOST`: Jaeger agent 所在的主机名
*   `JAEGER_AGENT_POR`: Jaeger 代理监听的端口
*   `JAEGER_SERVICE_NAME`: 我们的应用程序的名称

接下来，我们配置跟踪器并提供日志记录实现。如果可用的`ConsoleLogger`不够，则可以实现自定义日志记录解决方案。对于具有统一日志记录层的基于容器的应用程序，ConsoleLogger 应该足够了。

当我们设置了跟踪器时，我们希望将跨度添加到我们想要被检测的功能中。下面的代码就是这样做的:

```cpp
auto responder::respond(const http_request &request, status_code status,
                        const json::value &response) -> void {
  auto span = opentracing::Tracer::Global()->StartSpan("respond");
  // ...
}
```

此跨度稍后可用于在给定函数内创建子跨度。它也可能作为参数传播到更深的函数调用。这是它的样子:

```cpp
auto responder::prepare_response(const std::string &name, const std::unique_ptr<opentracing::Span>& parentSpan)
    -> std::pair<status_code, json::value> {
  auto span = opentracing::Tracer::Global()->StartSpan(
        "prepare_response", { opentracing::ChildOf(&parentSpan->context()) });
  return {status_codes::OK,
          json::value::string(string_t("Hello, ") + name + "!")};
}

auto responder::respond(const http_request &request, status_code status)
    -> void {
  auto span = opentracing::Tracer::Global()->StartSpan("respond");
  // ...
  auto response = this->prepare_response("Dominic", span);
  // ...
}
```

上下文传播发生在我们调用`opentracing::ChildOf`函数时。我们也可以使用`inject()`和`extract()`调用通过网络调用传递上下文。

# 使用服务网格连接服务

微服务和云原生设计都有自己的问题。不同服务之间的通信，可观察性，调试，速率限制，身份验证，访问控制和 A/B 测试即使在有限数量的服务下也可能具有挑战性。当服务数量上升时，上述需求的复杂性也会上升。

这就是服务网格进入竞争的地方。简而言之，服务网格会利用一些资源 (运行控制平面和边架所必需的) 来解决上述挑战的自动化和集中控制的解决方案。

## 引入服务网格

我们在本章简介中提到的所有要求过去都是在应用程序本身内编码的。事实证明，许多可能是抽象的，因为它们在许多不同的应用程序之间共享。当您的应用程序由许多服务组成时，向所有服务添加新功能将开始变得昂贵。使用服务网格，您可以从单个点控制这些功能。

由于容器化的工作流已经抽象了一些运行时和一些网络，因此服务网格将抽象带到了另一个层次。这样，容器内的应用程序只知道在 OSI 网络模型的应用程序级别发生了什么。服务网格处理较低的级别。

设置服务网格可让您以新的方式控制所有网络流量，并使您更好地了解此流量。依赖关系变得可见，流量、形状和流量也是如此。

不仅是由服务网格处理的流量。其他流行的模式，如电路断开、速率限制或重试，不必由每个应用程序实现并单独配置。这也是可以外包给服务网格的功能。同样，A/B 测试或金丝雀部署是服务网格能够满足的用例。

如前所述，服务网格的好处之一是更好的控制。它的体系结构通常由用于外部流量的可管理的边缘代理和通常作为每个微服务的边车部署的内部代理组成。这样，网络策略可以写成代码，并与所有其他配置一起存储在一个地方。无需为要连接的两个服务切换相互 TLS 加密，只需在服务网格配置中启用该功能一次。

接下来，我们将介绍一些服务网格解决方案。

## 服务网格解决方案

这里描述的所有解决方案都是自托管的。

### Istio

Istio 是一个强大的服务网格工具集合。它允许您通过将 Envoy 代理部署为 sidecar 容器来连接微服务。由于 Envoy 是可编程的，因此 Istio 控制平面的配置更改会传达给所有代理，然后代理相应地重新配置自己。

特使代理除其他外，负责处理加密和身份验证。使用 Istio，在大多数时间内，在服务之间启用相互 TLS 需要在配置中进行单个切换。如果您不希望所有服务之间的 mtl，则还可以选择那些需要这种额外保护的服务，同时允许其他所有服务之间的未加密流量。

Istio 还有助于可观察性。首先，Envoy 代理导出与 Prometheus 兼容的代理级度量。还有 Istio 导出的服务级度量和控制平面度量。接下来，有描述网格内流量的分布式跟踪。Istio 可以为不同的后端提供跟踪: Zipkin，Jaeger，Lightstep 和 Datadog。最后，还有 Envoy 访问日志，这些日志以类似于 Nginx 的格式显示每个调用。

可以使用交互式 web 界面 Kiali 可视化网格。这样，您可以看到服务的图表，包括是否启用了加密、不同服务之间的流量大小或每个服务的健康检查状态等信息。

Istio 的作者声称这个服务网格应该兼容不同的技术。在撰写本文时，最好的记录，最好的集成和最好的测试是与 Kubernetes 的集成。其他受支持的环境是本地，通用云，Mesos 和带有 Consul 的 Nomad。

如果您在关注合规性的行业 (例如金融机构) 工作，那么 Istio 可以在这些方面提供帮助。

#### 特使

虽然 Envoy 本身不是服务网格，但由于其在 Istio 中的使用，因此在本节中值得一提。

Envoy 是一个服务代理，其行为与 Nginx 或 HAProxy 非常相似。主要区别在于它可以即时重新配置。这是通过 API 以编程方式发生的，不需要更改配置文件，然后重新加载守护程序。

关于特使的有趣事实是它的表现和受欢迎程度。根据 SolarWinds 进行的测试，Envoy 在作为服务代理的性能方面击败了竞争对手。该竞赛包括 HAProxy、Nginx、Traefik 和 AWS 应用程序负载平衡器。Envoy 比 Nginx，HAProxy，Apache 和 Microsoft IIS 等已建立的领导者年轻得多，但这并没有阻止 Envoy 进入最常用的 web 服务器的前十名。

### Link

在 Istio 成为服务网格的代名词之前，该字段由 Linkerd 表示。由于最初的 Linkerd 项目旨在与平台无关并针对 Java VM，因此在命名方面有些混乱。这意味着它资源丰富，而且经常缓慢。更新的版本称为 Linkerd2，已被重写以解决这些问题。Linkerd2 与原始 Linkerd 相反，仅专注于 Kubernetes。

Linkerd 和 Linkerd2 都使用自己的代理解决方案，而不是依赖于 Envoy 等现有项目。这样做的理由是，专用代理 (相对于通用特使) 提供了更好的安全性和性能。Linkerd2 的一个有趣的功能是，开发它的公司也提供付费支持。

### 领事服务网格

服务网格空间的最新添加是 Consul 服务网格。这是 HashiCorp 的产品，HashiCorp 是一家知名的云公司，以 Terraform，Vault，Packer，Nomad 和 Consul 等工具而闻名。

就像其他解决方案一样，它具有 mTLS 和流量管理功能。它被宣传为多云、多数据中心和多区域网格。它与不同的平台，数据平面产品和可观察性提供商集成在一起。在撰写本文时，由于主要受支持的平台是 Nomad 和 Kubernetes，而受支持的代理是内置代理或 Envoy，因此现实更为温和。

如果您正在考虑将 Nomad 用于您的应用程序，那么 Consul service mesh 可能是一个不错的选择，并且非常适合，因为两者都是 HashiCorp 产品。

# 去 GitOps

本章我们要讨论的最后一个主题是 GitOps。尽管这个词听起来很新潮，但它背后的想法并不完全是新颖的。它是众所周知的**连续集成**/**连续部署** (**CI**/**CD**) 模式的扩展。或者也许扩展不是一个好的描述。

虽然 CI/CD 系统通常旨在非常灵活，但 GitOps 力求最大程度地减少可能的集成数量。两个主要的常量是 Git 和 Kubernetes。Git 用于版本控制、发布管理和环境分离。Kubernetes 被用作标准化和可编程的部署平台。

这样，CI/CD 管道变得几乎透明。这与命令代码处理构建的所有阶段的方法相反。为了允许这样的抽象级别，您通常需要以下内容:

*   基础设施作为代码，允许自动部署所有必要的环境
*   具有功能分支和拉取请求或合并请求的 Git 工作流
*   声明式工作流配置，在 Kubernetes 中已经可用

## GitOps 的原则

由于 GitOps 是已建立的 CI/CD 模式的扩展，因此区分两者可能不是很清楚。以下是一些 GitOps 原则，这些原则将这种方法与通用 CI/CD 区分开来。

### 声明性描述

经典 CI/CD 系统与 GitOps 之间的主要区别在于操作方式。大多数 CI/CD 系统都是必不可少的: 它们包括一系列步骤，以使管道成功。

甚至管道的概念也是必不可少的，因为它意味着一个具有条目，一组连接和一个接收器的对象。某些步骤可以并行执行，但是只要存在依赖关系，过程就必须停止并等待依赖步骤完成。

在 GitOps 中，配置是声明式的。这是指系统的整个状态-应用程序，它们的配置，监视和仪表板。它都被视为代码，赋予它与常规应用程序代码相同的功能。

### Git 中系统的状态版本

由于系统的状态是用代码编写的，因此您可以从该事实中获得一些好处。诸如更轻松的审核，代码审查和版本控制之类的功能现在不仅适用于应用程序代码。结果是，万一出现任何问题，恢复到工作状态需要一个`git revert`命令。

您可以使用 Git 的签名提交以及 SSH 和 GPG 密钥的强大功能来控制不同的环境。通过添加一个门控机制，确保只有符合要求标准的提交可以被推送到存储库，您还可以消除许多意外错误，这些错误可能是由于使用`ssh`或`kubectl`手动运行命令而导致的。

### 可审计

您存储在版本控制系统中的所有内容都变得可审核。在引入新代码之前，您需要执行代码审查。当您注意到一个错误时，您可以还原引入它的更改或返回到最后一个工作版本。您的存储库成为有关整个系统的唯一真相。

当应用于应用程序代码时，它已经很有用了。但是，扩展了审计配置、辅助服务、指标、仪表板甚至部署策略的能力，使其功能更加强大。你不再需要问自己，“*好的，那么为什么这个配置最终会在生产中结束？*” 你所要做的就是检查 Git 日志。

### 与已建立的组件集成

大多数 CI/CD 工具引入专有配置语法。詹金斯使用詹金斯 DSL。每个流行的 SaaS 解决方案都使用 YAML，但是 YAML 文件彼此不兼容。如果不重写你的管道，你就不能从特拉维斯切换到 CircleCI 或从 CircleCI 切换到 GitLab CI。

这有两个缺点。一是明显的供应商锁定。另一个是需要学习使用给定工具的配置语法。即使您的大部分管道已经在其他地方定义 (shell 脚本，Dockerfiles 或 Kubernetes 清单)，您仍然需要编写一些粘合代码来指示 CI/CD 工具使用它。

GitOps 不同。在这里，您不会编写明确的指令或使用专有语法。相反，您可以重用其他通用标准，例如 Helm 或 Kustomize。学习的东西更少，迁移过程也更舒适。此外，GitOps 工具通常与 CNCF 生态系统中的其他组件很好地集成在一起，因此您可以将部署指标存储在 Prometheus 中，并可以使用 Grafana 进行审核。

### 防止配置漂移

当给定系统的当前状态与存储库中所述的所需状态不同时，就会发生配置漂移。多种原因导致了配置漂移。

例如，让我们考虑具有基于 VM 的工作负载的配置管理工具。所有虚拟机都以相同状态启动。二 1212 首次运行时，它将机器带到所需的状态。但是，如果默认情况下在这些计算机上运行自动更新代理，则该代理可能会自行更新某些软件包，而无需考虑二 1212 的所需状态。此外，由于网络连接可能很脆弱，因此某些计算机可能会更新到软件包的较新版本，而另一些则不会。

更新的软件包之一可能与应用程序在极端情况下所需的固定软件包不兼容。这样的情况会破坏二 1212 工作流程，使您的机器处于无法使用的状态。

使用 GitOps，代理始终在系统内部运行，以跟踪系统的当前状态和所需状态。如果当前状态突然与所需状态不同，则代理可能会对其进行修复或发出有关配置漂移的警报。

防止配置漂移为您的系统增加了另一层自我修复。如果您正在运行 Kubernetes，则在 pod 级别上已经具有自我修复功能。每当一个 pod 发生故障时，就会在其位置重新创建另一个 pod。如果您在下面使用可编程的基础架构 (如云提供商或 OpenStack 内部部署)，您还具有节点的自我修复功能。使用 GitOps，您可以获得工作负载及其配置的自我修复。

## GitOps 的好处

可以想象，GitOps 的上述功能提供了几个好处。这里有一些。

### 提高生产率

CI/CD 管道已经自动执行了许多常规任务。他们通过帮助获得更多部署来减少交付时间。GitOps 添加了一个反馈循环，可以防止配置漂移并允许自我修复。这意味着您的团队可以更快地发货，并且不必担心引入潜在的问题，因为它们很容易恢复。反过来，这意味着开发吞吐量会增加，并且您可以更快，更放心地引入新功能。

### 更好的开发人员体验

使用 GitOps，开发人员不必担心构建容器或使用 kubectl 来控制集群。部署新功能只需要使用 Git，这在大多数环境中已经是一个熟悉的工具。

这也意味着入职更快，因为新员工不需要学习很多新工具就可以提高工作效率。GitOps 使用标准且一致的组件，因此在操作端引入更改不应影响开发人员。

### 更高的稳定性和可靠性

使用 Git 存储系统的状态意味着您可以访问审计日志。此日志包含所有引入的更改的描述。如果您的任务跟踪系统与 Git 集成 (这是一个很好的做法)，您通常可以分辨出哪个业务功能与系统的更改有关。

使用 GitOps，无需允许手动访问节点或整个群集，从而减少了因运行无效命令而导致意外错误的机会。那些进入系统的随机错误很容易通过使用 Git 强大的还原功能来修复。

从严重灾难 (例如失去整个控制机) 中恢复也容易得多。它所需要的只是设置一个新的干净群集，在那里安装 GitOps 运算符，并将其与您的配置指向存储库。过了一会儿，你就有了以前生产系统的精确复制品，所有这些都没有人工干预。

### 提高安全性

减少对集群和节点的访问需求意味着提高了安全性。就丢失或被盗的钥匙而言，不必担心。您可以避免这样的情况，即即使此人不再在团队 (或公司) 工作，也可以保留对您生产环境的访问权限。

当涉及到对系统的访问时，单点真相由 Git 存储库处理。即使恶意行为者决定在您的系统中引入后门，所需的更改也将经过代码审查。当您的存储库使用 GPG 签名的提交并进行强验证时，冒充另一个开发人员也更具挑战性。

到目前为止，我们主要从开发和运营的角度介绍了收益。但是 GitOps 也使业务受益。它提供了系统中的业务可观察性，这是以前很难实现的。

跟踪给定版本中存在的功能很容易，因为它们都存储在 Git 中。由于 Git 提交了指向任务跟踪器的链接，因此业务人员可以获取预览链接，以查看应用程序在各个开发阶段的外观。

它还提供了清晰的信息，可以回答以下常见问题:

*   生产中正在运行什么？
*   上次发行时哪些票证已解决？
*   哪个更改可能是服务降级的原因？

所有这些答案的问题甚至可以在友好的仪表板中显示。自然，仪表板本身也可以存储在 Git 中。

## GitOps 工具

GitOps 空间是一个新的且不断增长的空间。已经有可以认为是稳定和成熟的工具。以下是一些最受欢迎的。

### FluxCD

FluxCD 是 Kubernetes 的自以为是 GitOps 运算符。选定的集成提供了核心功能。它使用 Helm 图表和 Kustomize 来描述资源。

它与 Prometheus 的集成为部署过程增加了可观察性。为了帮助维护，FluxCD 具有 CLI。

### ArgoCD

与 FluxCD 不同，它提供了更广泛的工具选择。如果您已经在使用 Jsonnet 或 Ksonnet 进行配置，这可能会很有用。与 FluxCD 一样，它与 Prometheus 集成在一起，并具有 CLI。

在撰写本书时，ArgoCD 是比 FluxCD 更受欢迎的解决方案。

### 詹金斯 X

与名称可能暗示的相反，Jenkins X 与著名的 Jenkins CI 系统没有太多共同点。它得到了同一家公司的支持，但是 Jenkins 和 Jenkins X 的整个概念完全不同。

虽然其他两个工具是故意的小而独立的，但 Jenkins X 是一个复杂的解决方案，具有许多集成和更广泛的范围。它支持自定义构建任务的触发，使其看起来像是经典 CI/CD 系统和 GitOps 之间的桥梁。

# 摘要

恭喜你到了章的结尾!使用现代 C ++ 并不局限于理解最近加入的语言特性。您的应用程序将在生产中运行。作为架构师，确保运行时环境符合要求也是您的选择。在前几章中，我们描述了分布式应用程序中的一些流行趋势。我们希望这些知识将帮助您决定哪一种最适合您的产品。

使用云原生会带来很多好处，并且可以使您的工作流程中的很大一部分自动化。将定制工具切换到行业标准可以使您的软件更具弹性，更易于更新。在本章中，我们介绍了流行的云原生解决方案的优缺点和用例。

有些，例如使用 Jaeger 进行分布式跟踪，为大多数项目带来了直接的好处。其他人，如 Istio 或 Kubernetes，在大规模行动中表现最好。阅读本章后，您应该有足够的知识来决定将云原生设计引入您的应用程序是否值得付出代价。

# 问题

1.  在云中运行应用程序和使它们成为云原生应用程序有什么区别？
2.  您如何在本地运行云原生应用程序？
3.  Kubernetes 的最小高可用集群大小是多少？
4.  哪个 Kubernetes 对象表示允许网络连接的微服务？

5.  为什么在分布式系统中日志记录不足？
6.  服务网格如何帮助构建安全系统？
7.  GitOps 如何提高生产率？
8.  监控的标准 CNCF 项目是什么？

# 进一步阅读

*   Northern T0, 'Mastering KubernetesReuters T1, “What is the best way to do this?” https://www.packtpub.com/product/ mastering-kubernetes-third edition/9781839211256became T3
*   *Mastering Distributed Tracing* : [https://www.packtpub.com/product/ mastering-distributed-tracing/9781788628464](https://www.packtpub.com/product/mastering-distributed-tracing/9781788628464)
*   *掌握服务网格*: [https://www.packtpub.com/product/ 掌握-服务网格/9781789615791](https://www.packtpub.com/product/mastering-service-mesh/9781789615791)